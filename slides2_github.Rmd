---
title: "Introduction to Research Methods"
author: "Stijn Masschelein"
date: "`r format(Sys.Date(), '%B, %Y')`"
output:
  html_document:
    css: styles.css
    number_sections: yes
    highlight: kate
    theme: readable
    toc: yes
  slidy_presentation:
    css: styles_presentation.css
    font_adjustment: +1
    highlight: kate
    fig.width: 6
    fig.height: 4
---

```{r render, eval=FALSE, echo=FALSE}
rmarkdown::render("slides2.Rmd", output_format = "html_document",
                  output_file = "lecture_notes2.html")
rmarkdown::render("slides2.Rmd", output_format = "slidy_presentation",
                  output_file = "lecture_slides2.html")

# CEO-firm matching
## New theory

<div class="content_slide">
\[
V_1(n) - V_0(n) = C V_0(n) T(m)
\]

- $n$ is the rank of the size of the firm, $m$ is the rank of the talent of the 
  CEO. 
- $n = 1$ is the largest firm, $m = 1$ is the most talented CEO.

</div>

<div class="notes">
The model is the second one presented in Edmans and Gabraix (2016). A more
rigorous proof is shown in [Tervio (2008)](https://www.aeaweb.org/articles?id=10.1257/aer.98.3.642)

The increase in value of a company, $V_1 - V_0$, $n$, depends on the (T)alent of
CEO (m). In this setup a manager again has a larger impact in the largest firm
(V).
</div>

## Firm's decision

<div class="content_slide">
\[
\max_{m} CV_0(n)T(m) - w(m) \\
m = n\ \land\ w(m) \geq w_0
\]
</div>

<div class="notes">
- In this theory, the firm is taking the decision now, not the CEO. The decision
  is about which manager to hire and how much to pay them. There is no decision
  about labour or capital.
- Because more talented CEOs have a larger impact in larger firms, in       
  equilibrium, the most talented CEO should work for the largest firm.
- The inequality constraint says that CEOs can earn $w_0$ somewhere else.
</div>

## Three firms - three CEOs model

<div class="content_slide">
\[
CV_0(1) T(1) - w(1) \geq CV_0(1) T(2) - w(2) \\
CV_0(1) T(1) - w(1) \geq CV_0(1) T(3) - w(3)
\]

\[
CV_0(2) T(2) - w(2) \geq CV_0(2) T(3) - w(3)
\]
</div>

<div class="notes">
- The largest firm, $n = 1$, will only hire the best CEO if the supplus created
  by the CEO minus their wage is higher than hiring any other CEO.
</div>

## Three firms - three CEOs model

<div class="content_slide_wide">
\[
CV_0(1) T(1) - w(1) \geq CV_0(1) T(2) - w(2) \\
CV_0(2) T(2) - w(2) \geq CV_0(2) T(3) - w(3)
\]

\[
CV_0(1) T(1) - w(1) + CV_0(2) T(2) - \color{blue}{w(2)}
\geq CV_0(1) T(2) - \color{blue}{w(2)} + CV_0(2) T(3) - w(3)
\\
CV_0(1) T(1) - w(1) \geq C(V_0(1) - V_0(2))T(2) + C(V_0(2) - 
\color{blue}{V_0(1)})T(3) + \color{blue}{CV_0(1)T(3)}- w(3)
\\
CV_0(1) T(1) - w(1) \geq 
\color{blue}{C(V_0(1) - V_0(2))(T(2) - T(3))} + CV_0(1)T(3) - w(3)
\]
</div>

<div class="notes">
Because $V_0(1) > V_0(2)$ and $T_0(2) > T_0(3)$, we can delete the first term 
on the right hand side. So, the two inequalities give us the third inequality
from the previous slide for free.
</div>

## N firms - N CEOs model

<div class="content_slide">
\[
CV_0(n) T(n) - w(n) \geq CV_0(n) T(n+1) - w(n+1) \\
w(n) \geq w_0 \\
\forall n = 1,.., N-1
\]

Firm $n$ will pay just enough so that manager $m = n$ is too expensive for
firm $n - 1$. 

\[
CV_0(n + 1) (T(n) - T(n+1)) + w(n + 1) = w(n) \\
\forall n = 1,.., N-1
\]

> The original papers go further with the derivations. However, this is a recipe
we can give to `R` with some further assumptions. So let's build some fake data
in `R`
</div>

<div class="notes">
I hope the three firm model helped you to get some of the intuition behind 
the model without resorting to too complicated maths. Hopefully, you see how
we can extrapolate this deriviation to N firms and N CEOs. If the literature
points you to a mathematical model and you want to understand it better. 
Breaking it down to a simpler model with only 2 or 3 firms can be very 
illuminating.

The basic idea is that a larger firm will pay more for a CEO than a smaller
firm. If the larger firm wants to make sure that a the more talented CEO 
works for them, they need to pay the CEO a high enough wage. The difference
between the two wages for a talented CEO will be the surplus that the CEO would
create in the smaller firm. So the smaller firm will not be willing to poach the
more talented CEO because the costs (higher wage) would outweight the benefit
(higher surplus).

The original theoretical papers also need these extra assumptions. The  goal
is to show that sometimes you do not need all the fancy math when you can
write a computer program to do the work for you. We will use similar 
assumptions as the original papers but but them in an `R` program.
</div>

# Simulating data

## Packages
<div class="content_slide">
```{r, echo = TRUE, eval = FALSE}
library(tidyverse)
library(ggplot2)
```
</div>

<div class="notes">
These are just the packages. `tidyverse` helps us dealing with data manipulation
and organisation. `ggplot2` helps with making graphs.
</div>


## Simulating $V_0$ and $T$

<div class="content_slide">
```{r size_talent, echo=TRUE}
obs = 500
size_rate = 1; talent_rate = 2/3; 
C = 10; scale = 600; w0 = 0; 
n = c(1:obs)
size = scale * n ^ (-size_rate)
talent = - 1/talent_rate * n ^ (talent_rate)
```
</div>

<div class="notes">
The first three lines define the variables we are going to deal with. `obs` is
the number of observations or the number of firms and CEOs. `size_rate` is 
a parameter that controls the size of firms. A value of 1 means that firms
have constant returns to size. The same assumption as in the previous lecture.
`talent_rate` is something similar for the Talent of the CEOs. A larger number
for both rate parameters implies that differences between sizes or CEOs become
larger at the top.
`C` is the $C$ constant in the model. `scale` is an additional parameter that
helps me scale the size of the firms so that I get similar numbers as the real
data. (This is not necessarily a fudge. The theory does not say anything about 
whether we should measure firm size in USD, in AUD or in CNY. So the scale we
use is arbitrary.) `w0` is the base wage for the least talented CEO.

`n` is an `R` vector of length `obs` (i.e. 500) with values from 1 to `obs`. So
it is the rank in size and talent for each firm and CEO. 

Size and talent follow an exponential distribution which has some theoretical
motivation given in the originial papers. If that interests you, please go 
have a look but it goes beyond what we need today. What we do is give a value
for the size of each firm from 1 to 500 and for the talent of each CEO from 1 to
500. We will visualise these distributions to give a better idea of what these
distributions look like.
</div>

## Calculating compensation

<div class="content_slide">

```{r, echo = TRUE}
wage = rep(NA, obs)
wage[obs] = w0
for (i in (obs - 1):1){
  wage[i] = wage[i + 1] + 1/scale * C * size[i + 1] * 
    (talent[i] - talent[i + 1])
}
```
</div>

<div class="notes">
From our theory slide, we can also calculate the wage of each CEO-firm
combination. We start with creating a wage vector with 500 `NA` values. At the
last (500) position of the vector, we set the wage equal to `w0` for the least
talented CEO and the smallest firm. Than for each firm (we go from `i = 499` 
to `i = 1`), we set the wage as the wage of the smaller firm - less talented 
CEO plus the surplus our CEO would generate in the smaller firm.
</div>


## Create a dataset

<div class="content_slide">
And put everything in a dataset (or `tibble` in `tidyverse`)

```{r, echo = TRUE}
fake_data = tibble(
  n = n,
  size = size,
  talent = talent,
  wage = wage
) 
```
</div>

## Visualisations

<div class="content_slide">
```{r, echo = TRUE, fig.width=6, fig.height=5}
qplot(data = fake_data, x = size, y = wage) 
```
</div>

<div class="notes">
`qplot` for *q*uick *plots*. 
It's not perfect but the plot does follow a similar pattern as what we found
in the first lecture. Most important, we predict a non-linear relationship.
</div>

## Talent - Size relation

<div class="content_slide">
```{r, echo = TRUE, fig.width=6, fig.height=5}
qplot(data = fake_data, y = talent, x = size)
```
</div>

<div class="notes">
This picture shows us which assumptions were necessary for this to work. We
see that the difference in talent at the top of the distribution is not that
large, the difference in firm sizes is much more pronounced and is driving the
difference in wages.
</div>

## Functions in `R`

<div class="content_slide_wide">
```{r, echo = TRUE}
create_fake_data = function(obs = 500, size_rate = 1, talent_rate = 1,
                            w0 = .001, C = 1){
  scale = 600
  n = 1:obs
  size = scale * n ^ (-size_rate) 
  talent =  -1/talent_rate * n ^ talent_rate
  wage = rep(NA, obs)
  wage[obs] = w0 
  for (i in (obs - 1):1){
    wage[i] =  wage[i + 1] + 1/scale * C * size[i + 1] * 
      (talent[i] - talent[i + 1])
  }
  fake_data = tibble(n = n, size = size, talent = talent, wage = wage)
  return(fake_data)
}
fake_data = create_fake_data(talent_rate = 2/3, C = 0.01)
```
</div>

<div class="notes">
A very powerful feature of `R` is that we can write our own functions. This 
allows us to create fake data with different parameters quickly. In general, if
we can make the computer do the work for us, we wil probably be more efficient.
Making functions is just the recipe we give `R` so that it can do a lot of 
calculations multiple times.

Functions are created with the `function` function. In between brackets, you
define the parameter you want to use in your functions and their defaults. 
In between the curly braces `{}` you tell `R` what it should do with those
parameters. Ideally, you should not rely on any parameter or data that is not
defined in your function. `R` has some liberal defaults which might give you
unexpected results if you do that. You can rely on external functions.
</div>


## Experimenting with your model

<div class="content_slide_wide">
```{r, echo = TRUE}
data1 = create_fake_data(obs = 500, talent_rate = 2/3, C = .01) %>%
  mutate(talent_rate = "low talent", C = "low effect")
data2 = create_fake_data(obs = 500, talent_rate = 1, C = .01) %>%
  mutate(talent_rate = "high talent", C = "low effect")
data3 = create_fake_data(obs = 500, talent_rate = 2/3, C = .015) %>%
  mutate(talent_rate = "low talent", C = "high effect")
data4 = create_fake_data(obs = 500, talent_rate = 1, C = .015) %>%
  mutate(talent_rate = "high talent", C = "high effect")
data_exp = bind_rows(data1, data2, data3, data4)
data_exp
```
</div>

<div class="notes">
We can create new datasets where the talent_rate is increased to 1 (high talent)
and the effect of CEOs on the surplus is increased to .015 instead of .10. 
I choose .10 in this simulation instead of 10 so that all values can be 
interpreted in billion USD to mimic the real data. Again, that is a rather
arbitrary scaling issue.
</div>


## A more complicated plot with `ggplot`

<div class="content_slide">
```{r, echo = TRUE, fig.width=6, fig.height=3.5}
plot_exp = ggplot(data_exp, aes(x = size, y = wage)) +
  geom_point() + 
  geom_line(colour = "blue") +
  facet_grid(talent_rate ~ C)
print(plot_exp)
```
</div>

## Why simulating data?

<div class="content_slide">
- Visualising your theory
- Experimenting and understanding statistical tests 
- Experimenting with statistical approaches without peaking at your data.
</div>

<div class="notes">
- Visualising can help you sharpen your intuition for your theory and for
  which values are reasonable and which are not.
- You can simulate variables and causal structures that you cannot observe.
  - see also this week's homework
- You don't want to just decide on which statistical test to use because it 
  gives you the "right" answer. If you want to experiment with different 
  statistical models, you can do that with simulated data.
</div>

# Linear regression in `R`

## Notation 

<div class="content_slide">
\[\mathbf{y} = a + b_1 \mathbf{x_1} + ... + b_n \mathbf{x_n} 
                 + \mathbf{\epsilon} \]

\[\mathbf{y} = a + \mathbf{b} X + \mathbf{\epsilon} \]

\[\mathbf{y} \sim \mathcal{N}(a + \mathbf{b} X, \sigma) \]

```{r, eval = FALSE, echo = TRUE}
reg = lm(y ~ x1 + x2, data = my_data_set)
summary(reg)
```
</div>

## Linear regression with non-linear theories

<div class="content_slide_wide">

\[W = aV_0^b \implies ln(W) = ln(a) + b\ ln(V_0)\]


```{r linear_model, echo = TRUE, fig.width=6, fig.height=3.5}
plot_exp +
  scale_x_continuous(trans = "log", breaks = scales::pretty_breaks())  +
  scale_y_continuous(trans = "log", breaks = scales::pretty_breaks())
```
</div>

<div class="notes">
One reason why it does not work perfectly is that the reservation wage $w_0$ 
might be set to low in the simulation. You can see that the line is 
approximately linear for the largest firms. The real models assume that we have
$\infty$ firms and we are only interested in the largest firms, that is the most
right part of the graph. This explanation also illustrates how simulating from
your theory can help you to understand the theory better. 
</div>

## An example

### But first we need to get the data right.

<div class="content_slide_wide">
```{r, echo=TRUE}
folder = "~/Dropbox/R/wrds/data/"
us_comp = readRDS(paste0(folder, "us-compensation.RDS")) %>%
  rename(total_comp = tdc1)
us_value = readRDS(paste0(folder, "us-value.RDS")) %>%
  rename(year = fyear, market_value = mkvalt) 
summary(us_value$market_value)
```  
</div>

<div class="notes">
We have a bunch of observations with missing values (`NA`)
</div>

---------

### Some more data wrangling

<div class="content_slide_wide">
```{r, echo = TRUE}
glimpse(us_value)
us_value = us_value %>% distinct() 
```
</div>

<div class="notes">
For a reason that I have not figured out yet it looks like each observation
in the `us_value` dataset was downloaded twice.
</div>

---------

### Putting it all together
<div class="content_slide_wide">
```{r, echo = TRUE}
us_comp_value = left_join(select(us_comp, gvkey, year, total_comp),
                          us_value, by = c("year", "gvkey"))
glimpse(us_comp_value)
```
</div>

<div class="notes">
`left_join` is a function that joins two datasets together based on key 
variables. The join functions in the `tidyverse` can be a lifesaver if you
are working with multiple datasets that need to be matched together.

I am going to ignore the missing values but in your project you should not. You
should try to figure out why they are missing. The homework might illuminate 
one of the reasons.
</div>

---------

### Finally, the regression

<div class="content_slide_wide">
```{r, echo = TRUE}
reg = lm(log(total_comp) ~ log(market_value), data = us_comp_value)
# summary(reg)
print(summary(reg), digits = 1L)
```
</div>

<div class="notes">
`R` gives way to much information but I am just restricting the number of 
significant digits because that is my biggest concern. You can just use
`summary(reg)`.
</div>


--------

### Graphical check

<div class="content_slide_wide">
```{r, echo = TRUE, fig.width=5, fig.height=2.5}
plot_check = 
  ggplot(data = us_comp_value, aes(y = total_comp, x = market_value)) +
  geom_point(alpha = .25) +
  scale_x_continuous(trans = "log", 
                     labels = function(x)format(x/1000, digits = 2)) +
  scale_y_continuous(trans = "log", 
                     labels = function(x)format(x/1000, digits = 2)) + 
  ylab("compensation in $ millions") +
  xlab("market value in $ billions")
print(plot_check)
```
</div>

<div class="notes">
On the log-log scale, we see that there are some CEOs with very low compensation
compared to the bulk of the observations. In the next figure, we will just
ignore those.
</div>


---

<div class="content_slide_wide">
```{r, echo = TRUE, fig.width=5, fig.height=2.5}
plot_check2 = plot_check +  
  scale_y_continuous(trans = "log", limits = c(500, NA),
                     labels = function(x)format(x/1000, digits = 2)) 
print(plot_check2)
```
</div>

<div class="notes">
This looks pretty linear to me.
</div>


# Homework

## Simulations and visualisation

<div class="content_slide">
1. Check the conjecture that the log-log transformation makes the theoretical
  relation between wage-size almost linear for the largest firms.
2. Check whether the compensation-size relation is stable over time.
3. Simulate an exploding time series.
4. Causal structure and control variables
</div>

<div class="notes">
1. Linearity after log-log transformation
    - Adapt the simulation with the `fake_data()` function for the same 
      `C` and `talent_rate` values but with `obs = 10000`.
    - For each simulated dataset keep only the 500 largest firms.
        - You can use the variable `n` and the data manipulation functions we
          introduced last week
    - Plot the log(wage)-log(size) relation for the four different parameters
      spaces
    - What have you learned from this exercise?
</div>
 

<div class="notes">
2. One important assumption for the statistical tests of linear models is that
  the error terms are independently distributed. One could suspect that
  different years from the same firm have more similar error terms than the
  rest of the sample. In addition, a lot of tests in the accounting and finance
  literature use panel data (multiple firms, mutiple years) and assume that the
  relationship is more or less stable over time. Or we expect that the 
  relationship is stable until a major change in legislation.
    - Plot the log(compensation)-log(size) relation for each year using 
      the `facet_wrap()` function to assess whether the linear trend is visible in
      each year. Use the help resources if you want to know how to use 
      `facet_wrap()`.
    - Does the linear relation hold from year to year?
</div>


<div class="notes">
3. An exploding time series. When you deal with time series, you will talk about
   the stationarity property of a time series. To illustrate this issue, let's
   simulate two time series. The simplest time series have a structure that
   depends on the previous obervation and some independent random noise. For
   simplicity let's use normally distributed noise. We can then write.
   $$
   V_t = \beta V_{t-1} + \epsilon_t \\
   V_t \sim \mathcal{N}(\beta V_{t-1}, \sigma)
   $$
   I want you to generate two time series `x` and `y` with 365 obervations each. 
   For both time series, you can generate the noise as normally distribute with
   mean 0 and standard devation .5. For `x` $\beta = .9$ and for `y` $\beta =
   1.1$. For the first, value you can use the same random noise as above. Have
   a look at the `wage` script above. There are striking similarities with 
   time series.
   Put both variables in a dataset (`tibble`) and plot the two time series over
   time in two separate plots. This means you will have to have a variable that
   designates the time (i.e. the day).
</div>


<div class="notes">
4. A lot of you will deal with the problem of which control variables to
    introduce in your statistical model. I have a lot of opinions about this. 
    However for now, I just want to show you how simulations can help you decide
    whether you should include some variables. Let's assume you have the 
    following theoretical model in mind. You have your variable of interest x
    (= CEO experience) and you believe that x leads to more of y (firm profit). 
    You think that because a x leads to z (optimal investment policy) and z
    leads to y. So we have x --> z --> y as your causal model. If this is what
    you think than I would argue you should not control for z when you regress
    x on y.
    Let's put some numbers on the causal model. 
    $$
    x \sim \mathcal{N(0, 1)} \\
    z \sim \mathcal{N(1 + x, 1)} \\
    y \sim \mathcal{N(1 + z, 1)} \\
    $$
   In this causal model, more of x leads ultimately to more of y. Your tests 
   should reflect that. 
     - Simulate 1000 values of x, y, z for this causal structure.
     - Run the linear regression with z as dependent variable and x as 
       independent variable. 
     - Run the linear regression with y as dependent variable and x as 
       independent variable. 
     - Run the linear regression with y as dependent variable and x and z as
       independent variable.
     - What have you learned from this exercise?
</div>

</div>

